{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_SVD and SVD++.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1bH--9vRFb77AOxgTp199mWd89nT-khOb",
      "authorship_tag": "ABX9TyNGeJ6A8nKNnjmiEbBHOOFs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yordanovagabriela/recommendersys/blob/master/HW3_SVD_and_SVD%2B%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sfbnmep-dz-o",
        "colab_type": "text"
      },
      "source": [
        "# Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R_-gsp9d54b",
        "colab_type": "text"
      },
      "source": [
        "## Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wso5Sw0hn-Ky",
        "colab_type": "code",
        "outputId": "e032bd26-dccd-4724-f246-cd2613b958d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install surprise"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting surprise\n",
            "  Downloading https://files.pythonhosted.org/packages/61/de/e5cba8682201fcf9c3719a6fdda95693468ed061945493dea2dd37c5618b/surprise-0.1-py2.py3-none-any.whl\n",
            "Collecting scikit-surprise\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/da/b5700d96495fb4f092be497f02492768a3d96a3f4fa2ae7dea46d4081cfa/scikit-surprise-1.1.0.tar.gz (6.4MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.18.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from scikit-surprise->surprise) (1.12.0)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.0-cp36-cp36m-linux_x86_64.whl size=1678565 sha256=6c71af1517325cf827f7869ab960ad2ec241fcfdd96c7ac134ee5fd0ba416df1\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/fa/8c/16c93fccce688ae1bde7d979ff102f7bee980d9cfeb8641bcf\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.0 surprise-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KWbPBQmd9Hv",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gRRoGxfcn8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from surprise import SVD\n",
        "from surprise import SVDpp\n",
        "from surprise import KNNWithMeans\n",
        "from surprise import NormalPredictor\n",
        "\n",
        "from surprise import Dataset\n",
        "\n",
        "from surprise import accuracy\n",
        "\n",
        "from surprise.model_selection import GridSearchCV\n",
        "from surprise.model_selection import LeaveOneOut\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "from collections import defaultdict\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZfHYzSqeFRA",
        "colab_type": "text"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xE6vAAJxZiWl",
        "outputId": "4649df4d-dbc9-419e-c743-d764d890f2ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "data = Dataset.load_builtin('ml-100k')\n",
        "df = pd.DataFrame(data.raw_ratings)\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>881250949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>186</td>\n",
              "      <td>302</td>\n",
              "      <td>3.0</td>\n",
              "      <td>891717742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>377</td>\n",
              "      <td>1.0</td>\n",
              "      <td>878887116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>244</td>\n",
              "      <td>51</td>\n",
              "      <td>2.0</td>\n",
              "      <td>880606923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166</td>\n",
              "      <td>346</td>\n",
              "      <td>1.0</td>\n",
              "      <td>886397596</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>298</td>\n",
              "      <td>474</td>\n",
              "      <td>4.0</td>\n",
              "      <td>884182806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>115</td>\n",
              "      <td>265</td>\n",
              "      <td>2.0</td>\n",
              "      <td>881171488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>253</td>\n",
              "      <td>465</td>\n",
              "      <td>5.0</td>\n",
              "      <td>891628467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>305</td>\n",
              "      <td>451</td>\n",
              "      <td>3.0</td>\n",
              "      <td>886324817</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>6</td>\n",
              "      <td>86</td>\n",
              "      <td>3.0</td>\n",
              "      <td>883603013</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     0    1    2          3\n",
              "0  196  242  3.0  881250949\n",
              "1  186  302  3.0  891717742\n",
              "2   22  377  1.0  878887116\n",
              "3  244   51  2.0  880606923\n",
              "4  166  346  1.0  886397596\n",
              "5  298  474  4.0  884182806\n",
              "6  115  265  2.0  881171488\n",
              "7  253  465  5.0  891628467\n",
              "8  305  451  3.0  886324817\n",
              "9    6   86  3.0  883603013"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xia1z6XtNAAE",
        "colab_type": "text"
      },
      "source": [
        "# Split Data on Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL5bjhjWMjpx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set, test_set = train_test_split(data, test_size=.25, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b162HDx4MMhH",
        "colab_type": "text"
      },
      "source": [
        "# Create LOO Train and Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1-vSf_kens6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loo = LeaveOneOut(n_splits=1, random_state=1)\n",
        "for train, test in loo.split(data):\n",
        "    loo_train = train\n",
        "    loo_test = test\n",
        "\n",
        "loo_anti_testset = loo_train.build_anti_testset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEnJke1KNPub",
        "colab_type": "text"
      },
      "source": [
        "# Define Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8amFNSpgolM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mae(predictions):\n",
        "    return accuracy.mae(predictions, verbose=False)\n",
        "\n",
        "def rmse(predictions):\n",
        "    return accuracy.rmse(predictions, verbose=False)\n",
        "\n",
        "def hit_rate(top_n_predictions, left_out_predictions):\n",
        "    hits = 0\n",
        "    total = 0\n",
        "\n",
        "    for left_out in left_out_predictions:\n",
        "        user_id = left_out[0]\n",
        "        left_out_movie_id = left_out[1]\n",
        "\n",
        "        for movie_id, predicted_rating in top_n_predictions[int(user_id)]:\n",
        "            if (int(left_out_movie_id) == int(movie_id)):\n",
        "                hits += 1\n",
        "                break\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    return hits / total\n",
        "\n",
        "def average_reciprocal_hit_rank(top_n_predictions, left_out_predictions):\n",
        "    sum = 0\n",
        "    total = 0\n",
        "\n",
        "    for user_id, left_out_movie_id, actual_rating, estimated_rating, _ in left_out_predictions:\n",
        "        hit_rank = 0\n",
        "        rank = 0\n",
        "\n",
        "        for movie_id, predicted_rating in top_n_predictions[int(user_id)]:\n",
        "            rank = rank + 1\n",
        "            if (int(left_out_movie_id) == movie_id):\n",
        "                hit_rank = rank\n",
        "                break\n",
        "\n",
        "        if (hit_rank > 0) :\n",
        "            sum += 1.0 / hit_rank\n",
        "\n",
        "        total += 1\n",
        "\n",
        "    return sum / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ut057Exrlrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_n_predictions(predictions, n = 10, minimum_rating = 4.0):\n",
        "    top_n = defaultdict(list)\n",
        "\n",
        "    for user_id, movie_id, actual_rating, estimated_rating, _ in predictions:\n",
        "        if (estimated_rating >= minimum_rating):\n",
        "            top_n[int(user_id)].append((int(movie_id), estimated_rating))\n",
        "\n",
        "    for user_id, ratings in top_n.items():\n",
        "        ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[int(user_id)] = ratings[:n]\n",
        "\n",
        "    return top_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efGpzHFWR5CV",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwXwBoVef2xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_algorithm(algorithm, n = 10):\n",
        "  metrics = {}\n",
        "\n",
        "  print(\"\\t> Training ...\")\n",
        "  algorithm.fit(train_set)\n",
        "\n",
        "  print(\"\\t> Evaluating accuracy...\")\n",
        "  predictions = algorithm.test(test_set)\n",
        "\n",
        "  metrics[\"RMSE\"] = rmse(predictions)\n",
        "  metrics[\"MAE\"] = mae(predictions)\n",
        "\n",
        "  algorithm.fit(loo_train)\n",
        "  print(\"\\t> Evaluating top N with leave-one-out...\")\n",
        "  left_out_predictions = algorithm.test(loo_test) \n",
        "\n",
        "  all_predictions = algorithm.test(loo_anti_testset)\n",
        "  top_n_predictions = get_top_n_predictions(all_predictions, n, minimum_rating = 4.0)\n",
        "\n",
        "  print(\"\\t> Computing hit-rate and rank metrics...\")\n",
        "  metrics[\"HR\"] = hit_rate(top_n_predictions, left_out_predictions)\n",
        "  metrics[\"ARHR\"] = average_reciprocal_hit_rank(top_n_predictions, left_out_predictions)\n",
        "  return metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXYPX_aQ8Wog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_algorithms(algorithms, n = 10):\n",
        "  results = {}\n",
        "\n",
        "  for name in algorithms:\n",
        "    algorithm = algorithms[name]\n",
        "    print(\">> Evaluating {} ...\".format(name))\n",
        "    results[name] = evaluate_algorithm(algorithm, n)\n",
        "\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WoGI3B5rXoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretty_print(results):\n",
        "  print(\"{:<30} {:<10} {:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"ARHR\"))\n",
        "  for (name, metrics) in results.items():\n",
        "      print(\"{:30} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"ARHR\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knGvzM6rGuzi",
        "colab_type": "text"
      },
      "source": [
        "## Untuned\n",
        "### Default Parameters\n",
        "- SVD - `n_factors = 100`, `n_epochs  = 20`, `lr_all = 0.005`, `reg_all = 0.02`\n",
        "- SVD++ - `n_factors = 20`, `n_epochs  = 20`, `lr_all = 0.007`, `reg_all = 0.02`\n",
        "- KNNWithMeans - `sim_options = {'name': 'MSD', 'user_based': True}`\n",
        "\n",
        "*surprise.similarities.pearson_baseline()*:\\\n",
        "Compute the (shrunk) Pearson correlation coefficient between all pairs of users (or items) using baselines for centering instead of means. The shrinkage parameter helps to avoid overfitting when only few ratings are available\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqWm2R547rLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "algorithms = {}\n",
        "algorithms[\"SVD\"] = SVD(verbose=False)\n",
        "algorithms[\"SVD++\"] = SVDpp(verbose=False)\n",
        "algorithms[\"KNNWithMeans Item-Based\"] = KNNWithMeans(sim_options = {'name': 'pearson_baseline', 'user_based': False}, verbose=False)\n",
        "algorithms[\"KNNWithMeans User-Based\"] = KNNWithMeans(sim_options = {'name': 'pearson_baseline', 'user_based': True}, verbose=False)\n",
        "algorithms[\"Random\"] = NormalPredictor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm5CIKRC9Laz",
        "colab_type": "code",
        "outputId": "a769d967-3621-485b-da91-79a18dedef81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "results = evaluate_algorithms(algorithms)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Evaluating SVD ...\n",
            "\t> Training ...\n",
            "\t> Evaluating accuracy...\n",
            "\t> Evaluating top N with leave-one-out...\n",
            "\t> Computing hit-rate and rank metrics...\n",
            ">> Evaluating SVD++ ...\n",
            "\t> Training ...\n",
            "\t> Evaluating accuracy...\n",
            "\t> Evaluating top N with leave-one-out...\n",
            "\t> Computing hit-rate and rank metrics...\n",
            ">> Evaluating KNNWithMeans Item-Based ...\n",
            "\t> Training ...\n",
            "\t> Evaluating accuracy...\n",
            "\t> Evaluating top N with leave-one-out...\n",
            "\t> Computing hit-rate and rank metrics...\n",
            ">> Evaluating KNNWithMeans User-Based ...\n",
            "\t> Training ...\n",
            "\t> Evaluating accuracy...\n",
            "\t> Evaluating top N with leave-one-out...\n",
            "\t> Computing hit-rate and rank metrics...\n",
            ">> Evaluating Random ...\n",
            "\t> Training ...\n",
            "\t> Evaluating accuracy...\n",
            "\t> Evaluating top N with leave-one-out...\n",
            "\t> Computing hit-rate and rank metrics...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwxuQC6Cura5",
        "colab_type": "code",
        "outputId": "92b3bf8c-e2b4-48e7-bb78-70e9c008d152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "pretty_print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Algorithm                      RMSE       MAE        HR         ARHR      \n",
            "SVD                            0.9432     0.7422     0.0445     0.0162    \n",
            "SVD++                          0.9222     0.7237     0.0498     0.0186    \n",
            "KNNWithMeans Item-Based        0.9265     0.7235     0.0032     0.0019    \n",
            "KNNWithMeans User-Based        0.9454     0.7358     0.0032     0.0018    \n",
            "Random                         1.5287     1.2311     0.0223     0.0061    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwAJWIvLC7Y4",
        "colab_type": "text"
      },
      "source": [
        "As expected, SVD++ gives better results than SVD in accuracy and hit rate.\n",
        "\n",
        "The results given by item and user-based CF are rather more interesting. They achieve about the same accuracy (item-based gives slighly better results), but they totally fail at hit rate and reciprocal hit rank, i.e they are pretty goot at predicting ratings, but really bad at giving top-n recommendations. So, if we need to choose betwen item, user-based and random, we could consider using random as it gives way better results on hr and arhr.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ4W6JrJG17i",
        "colab_type": "text"
      },
      "source": [
        "## Tuned\n",
        "Now, lets try to tune the alogrithms and see if we can achieve better results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3yArAJ7V0tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_grid = {'n_factors':[50,100,150],\n",
        "              'n_epochs':[20,30],\n",
        "              'lr_all':[0.005,0.01],\n",
        "              'reg_all':[0.02,0.1]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9Dns08GxS7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gsSVD = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
        "gsSVD.fit(data)\n",
        "\n",
        "params = gsSVD.best_params['rmse']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAqwl22sw9Fn",
        "colab_type": "code",
        "outputId": "dde43897-9f3b-4c8a-9428-1e486f04fb36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"Best RMSE score achieved for SVD: {}\".format(gsSVD.best_score['rmse']))\n",
        "print(\"Parameters that gave the best RMSE score for SVD: {}\".format(params))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best RMSE score achieved for SVD: 0.921710712966808\n",
            "Parameters that gave the best RMSE score for SVD: {'n_factors': 150, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnnX6QE0xUL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gsSVDpp = GridSearchCV(SVDpp, param_grid, measures=['rmse', 'mae'], cv=3)\n",
        "gsSVDpp.fit(data)\n",
        "\n",
        "params = gsSVDpp.best_params['rmse']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMmxUvc3x9N-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Best RMSE score achieved for SVD: {}\".format(gsSVD.best_score['rmse']))\n",
        "print(\"Parameters that gave the best RMSE score for SVD: {}\".format(params))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvZSynCJFDsm",
        "colab_type": "code",
        "outputId": "75499f64-e309-4f2b-ba59-880df45acf2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "algorithms = {}\n",
        "algorithms[\"SVD Tuned\"] = SVD(n_factors=150, n_epochs=30, lr_all=0.01, reg_all=0.1, verbose=False)\n",
        "algorithms[\"SVD++ Tuned\"] = SVDpp(n_factors=150, n_epochs=30, lr_all=0.01, reg_all=0.1, verbose=False)\n",
        "results = evaluate_algorithms(algorithms)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Evaluating SVD Tuned ...\n",
            "\t> Training ...\n",
            "\t> Evaluating accuracy...\n",
            "\t> Evaluating top N with leave-one-out...\n",
            "\t> Computing hit-rate and rank metrics...\n",
            ">> Evaluating SVD++ Tuned ...\n",
            "\t> Training ...\n",
            "\t> Evaluating accuracy...\n",
            "\t> Evaluating top N with leave-one-out...\n",
            "\t> Computing hit-rate and rank metrics...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWFwQRF5N2sf",
        "colab_type": "code",
        "outputId": "7f22bfed-9359-48c0-d121-505d62ce5a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "pretty_print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Algorithm                      RMSE       MAE        HR         ARHR      \n",
            "SVD Tuned                      0.9185     0.7251     0.0403     0.0109    \n",
            "SVD++ Tuned                    0.9135     0.7205     0.0403     0.0113    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZfiK0KpS_yD",
        "colab_type": "text"
      },
      "source": [
        "The tuned versions of SVD and SVD++ give better results on accuracy, however the HR and ARHR are now lower. Which means that they are now worse in giving top-n recommendations.\n",
        "\n",
        "It will be useful to test other metrics such as diversity as it may turn out that the models give too obscure results opposed to the better accuracy they achieved."
      ]
    }
  ]
}